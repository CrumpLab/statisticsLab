\documentclass[]{book}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\hypersetup{unicode=true,
            pdftitle={Answering questions with data: Lab Manual},
            pdfauthor={Matthew J. C. Crump; Anajali Krishnan; Stephen Volz; Alla Chavarga; Jeffrey Suzuki},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{natbib}
\bibliographystyle{apalike}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{{#1}}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{{#1}}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{{#1}}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{{#1}}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{{#1}}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{{#1}}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{{#1}}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{{#1}}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{{#1}}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{{#1}}}
\newcommand{\ImportTok}[1]{{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{{#1}}}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{{#1}}}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{{#1}}}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{{#1}}}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{{#1}}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{{#1}}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{{#1}}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{{#1}}}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{{#1}}}}
\newcommand{\BuiltInTok}[1]{{#1}}
\newcommand{\ExtensionTok}[1]{{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{{#1}}}
\newcommand{\RegionMarkerTok}[1]{{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{{#1}}}}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{{#1}}}}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{{#1}}}}
\newcommand{\NormalTok}[1]{{#1}}
\usepackage{longtable,booktabs}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\newcommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}
  \title{Answering questions with data: Lab Manual}
  \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
  \author{Matthew J. C. Crump \\ Anajali Krishnan \\ Stephen Volz \\ Alla Chavarga \\ Jeffrey Suzuki}
  \preauthor{\centering\large\emph}
  \postauthor{\par}
  \predate{\centering\large\emph}
  \postdate{\par}
  \date{2018-07-20}

\usepackage{booktabs}
\usepackage{amsthm}
\makeatletter
\def\thm@space@setup{%
  \thm@preskip=8pt plus 2pt minus 4pt
  \thm@postskip=\thm@preskip
}
\makeatother

\usepackage{amsthm}
\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{lemma}{Lemma}[chapter]
\theoremstyle{definition}
\newtheorem{definition}{Definition}[chapter]
\newtheorem{corollary}{Corollary}[chapter]
\newtheorem{proposition}{Proposition}[chapter]
\theoremstyle{definition}
\newtheorem{example}{Example}[chapter]
\theoremstyle{definition}
\newtheorem{exercise}{Exercise}[chapter]
\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\newtheorem*{solution}{Solution}
\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\chapter*{Preface}\label{preface}
\addcontentsline{toc}{chapter}{Preface}

This lab manual involves tutorials and data-analysis problems using the
free statistics software R, as well as Excel, and SPSS. The goal is to
train students to be able to organize and analyze data common to
research in psychology, as well as to understand the ideas behind the
analyses so students can take creative approaches to answering questions
with data.

\section{R}\label{r}

\includegraphics[width=1.39in]{figures/rlogo}

R is primarily a computer programming language for statistical analysis.
It is \emph{free}, and \emph{open-source} (many people contribute to
developing it), and runs on most operating systems. It is a powerful
language that can be used for all sorts of mathematical operations,
data-processing, analysis, and graphical display of data. I even used R
to write this lab manual. And, I use R all the time for my own research,
because it makes data-analyis fast, efficient, transparent,
reproducible, and exciting.

Statistics Software

\begin{itemize}
\tightlist
\item
  \href{http://www-01.ibm.com/software/analytics/spss/}{SPSS}
\item
  \href{http://www.sas.com/en_us/home.html}{SAS}
\item
  \href{http://www.jmp.com}{JMP}
\item
  \href{http://www.r-project.org}{R}
\item
  \href{http://julialang.org}{Julia}
\item
  \href{http://www.mathworks.com/products/matlab/}{Matlab}
\end{itemize}

\section{Why R?}\label{why-r}

There are lots of different options for using computers to analyze data,
why use R?. The options all have pros and cons, and can be used in
different ways to solve a range of different problems. Some software
allows you to load in data, and then analyze the data by clicking
different options in a menu. This can sometimes be fast and convenient.
For example, once the data is loaded, all you have to do is click a
couple buttons to analyse the data! However, many aspects of
data-analysis are not so easy. For example, usually particular analyses
require that the data be formatted in a particular way so that the
program analyze it properly. Often times when a researcher wants to ask
a new question of an existing data set, they have to spend time
re-formatting the data. If the data is large, then reformattin by hand
is very slow, and can lead to errors. Another option, is to use a
scripting language to instruct the computer how reformat the data. This
is very fast and efficient. R provides the ability to everything all in
one place. You can load in data, reformat it any way you like, then
anlayze it anyway you like, and create beautiful graphs and tables
(publication quality) to display your findings. Once you get the hang of
R, it becomes very fast and efficient.

\section{Installing R and R Studio}\label{installing-r-and-r-studio}

Download and install R onto your computer. The R website is:
\url{http://www.r-project.org}

Find the download R using the link. This will take you to a page with
many different mirror links. You can click any of these links to
download a version of R that will work on your computer. After you have
installed R you can continue.

After you have installed R on your computer, you should want to install
another program called R studio. This program provides a user-friendly
interface for using R. You must already have installed R before you
perform this step. The R-studio website is: \url{http://www.rstudio.com}

Find the download link on the front-page, and then download R studio
desktop version for your computer. After you have installed R studio you
will be ready to start using R.

The website \href{http://www.r-fiddle.org}{R-fiddle} allows you to run R
scripts in the cloud, so you can practice R from your web-browser!

\section{R studio notes and tips}\label{r-studio-notes-and-tips}

\begin{figure}[htbp]
\centering
\includegraphics{figures/FigRstudio.pdf}
\caption{\label{fig:2rstudiod}The R-studio workspace}
\end{figure}

\subsection{Console}\label{console}

When you open up R studio you will see three or four main windows (the
placement of each are configurable). In the above example, the bottom
left window is the command line (terminal or console) for R. This is
used to directly enter commands into R. Once you have entered a command
here, press enter to execute the command. The console is useful for
entering single lines of code and running them. Oftentimes this occurs
when you are learning how to correctly execute a line of code in R. Your
first few attempts may be incorrect resulting in errors, but trying out
different variations on your code in the command line can help you
produce the correct code. Pressing the up arrow while in the console
will scroll through the most recently executed lines of code.

\subsection{Script Editor}\label{script-editor}

The top left corner contains the script editor. This is a simple text
editor for writing and saving R scripts with many lines. Several tabs
can be opened at once, with each tab representing a different R script.
R scripts can be saved from the editor (resulting in a .r file). Whole
scripts can be run by copy and pasting them into the console and
pressing enter. Alternatively, you can highlight portions of the script
that you want to run (in the script editor) and press command-enter to
automatically run that portion in the console (or press the button for
running the current line/section: green arrow pointing right).

\subsection{Workspace and History}\label{workspace-and-history}

The top right panel contains two tabs, one for the workspace and another
for history. The workspace lists out all of the variables and functions
that are currently loaded in R's memory. You can inspect each of the
variables by clicking on them. This is generally only useful for
variables that do not contain large amounts of information. The history
tab provides a record of the recent commands executed in the console.

\subsection{File, Plot, Packages, Help}\label{file-plot-packages-help}

The bottom-right window has four tabs for files, plots, packages, and
help. The files tab allows browsing of the computers file directory. An
important concept in R is the \textbf{current working directory}. This
is file folder that R points to by default. Many functions in R will
save things directly to this direct, or attempt to read files from this
directory. The current working directory can be changed by navigating to
the desired folder in the file menu, and then clicking on the more
option to set that folder to the current working directory. This is
especially important when reading in data to R. The current working
directory should be set to the folder containing the data to be inputted
into R. The plots tab will show recent plots and figures made in R. The
packages tab lists the current R libraries loaded into memory, and
provides the ability to download and enable new R packages. The help
menu is an invaluable tool. Here, you can search for individual R
commands to see examples of how they are used. Sometimes the help files
for individual commands are opaque and difficult to understand, so it is
necessary to do a google search to find better examples of using these
commands.

\section{Final comments}\label{final-comments}

In this course we will be using R as a tool to analyze data, and as a
tool to help us gain a better understanding of what our analyses are
doing. Throughout each lab we will show you how to use R to solve
specific problems, and then you will use the examples to solve homework
and lab assignments. R is a very deep programming language, and in many
ways we will only be skimming the surface of what R can do. Along the
way, there will be many pointers to more advanced techniques that
interested students can follow to become experts in using R for
data-analysis, and computer programming in general.

\chapter{Lab 1: Graphing Data}\label{lab-1-graphing-data}

{ The commonality between science and art is in trying to see profoundly
- to develop strategies of seeing and showing. ---Edward Tufte }

\section{General Goals}\label{general-goals}

\subsection{Other things if needed}\label{other-things-if-needed}

\section{R}\label{r-1}

\subsection{General Goals}\label{general-goals-1}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  A brief tour of R-studio
\item
  Some R basics
\item
  Graphing data in R
\item
  Graphing data using ggplot2
\end{enumerate}

\subsubsection{R basics Checklist}\label{r-basics-checklist}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Create a new R project
\item
  Execute commands in the console
\item
  Open and save new R script in the editor
\item
  Write a short script, and run the commands in the script in the
  console
\item
  View the contents of variables in the environment window
\item
  View the contents of variables using the console
\item
  Use the console like a calculator
\item
  Store numbers in variables
\end{enumerate}

\subsection{A clean start}\label{a-clean-start}

Good organization is key to data analysis. To explain, let me tell you a
story to help you avoid being like me when I started learning how to
analyze data. As a graduate student, I would collect data from
experiments I was running. I stored the data in different files in
different folders on my computer (all over the place, hard to remember
where sometimes). I would copy the data into excel so I could look at
it, do basic analysis, and reformat it so I could run statistics on the
data using programs like SPSS. I would often have many different
versions of excel spreadsheets with different versions of the data,
sometimes stored in different folders. I would have many different SPSS
outputs for each analysis I performed, sometimes in different locations.
I would create tables and graphs in new excel spreadsheets, and edit the
graphs in programs like Adobe Illustrator. All of these files would be
all over the place. Sometimes, weeks, or months, or years later, I would
revisit the data. After spending time to find it all, I would have to
retrace my steps, doing detective work to figure out what analyses I had
done. Ultimately, my previous work was so hard to understand, that I
would end up redoing the analysis again. This was a messy process, and
it took a lot of time. Wouldn't it be nice if everything was in one
place? R provides this solution.

\subsubsection{Making an R project}\label{making-an-r-project}

R projects are a convenient way to organize everything you do in R. To
create an R project in R-studio you can go to the file menu, and choose
``New Project\ldots{}''. Or, if you look in the top, right-side of the
screen, you should see a little blue cube with an R in it. This shows
your current R project. You can click this to create a new R project.

If you are using a lab computer, then insert a USB stick. Then click to
create a new R project. Navigate to your USB stick drive. Give your
project a name, like ``StatsLab''. This will create a new folder on your
USB stick. Inside the folder will be a new R project file.

Once you have loaded an R project, all of the new files that you make
will be saved in this project folder. You can also put data that you
want to analyse in this folder. Additionally, the output of your
analyses (including figures etc.) will be saved into this folder. This
great, because everything is in one place, and you know where that is.
When you want to return to work on your R project, you just have to load
it up. You can make as many R projects as you like as a way to organize
your work in R.

\subsection{R console}\label{r-console}

R does things using scripts, which involves typing in commands to R. To
begin, we will learn how to type in a command and execute it using the
console.

The console is an interface to using R. You type in a command, then
press enter to execute it. Then, R will show you the result in the
console.

The console should be located in the bottom-left window of R-studio. You
should see a tab that says ``Console''. If you do not see the console
window, then click on the word Console, and the window should appear.

Inside the console you will a bunch of text telling you what version of
R you are using. Scroll down to the bottom of the console and you should
see a blue arrow (\textgreater{}) followed by a cursor. If you click
into the console, then you will be able to type commands. For example,
click into the console and type 1+1, then press enter.

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{1+1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 2
\end{verbatim}

Above you should see two grey boxes. The first grey box is example code,
showing what I typed into the console (e.g., 1+1). The second grey box
is output given by R after pressing enter. You can see it gave the
answer 2.

\subsubsection{Using the console as a
calculator}\label{using-the-console-as-a-calculator}

R can be used just a like a calculator. Here are some examples:

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{7+100}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 107
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{43-23}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 20
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{34}\NormalTok{*}\DecValTok{4}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 136
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{22}\NormalTok{/}\DecValTok{2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 11
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{1}\NormalTok{+(}\DecValTok{2}\NormalTok{*}\DecValTok{3}\NormalTok{)+}\DecValTok{5}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 12
\end{verbatim}

Try using the R console as a calculator for yourself.

Using the console is a quick and easy way to enter one command at a
time. However, what if you want to enter more than one command? In this
case, we want to write a script. A script is a recipe of multiple
commands that tells R to do more than one thing, one after another.

\subsection{R editor}\label{r-editor}

We will use the R editor to write, save, and work on our scripts. The
editor appears in the top-left window of R-Studio. When you open new
scripts in R, you will see them appear as new tabs in the Editor window.

To open a new R script, look to the top left-hand side of R-studio. You
should see a white square with a green plus sign. Click this button, and
you can create a new R script.

The first thing that happens is a new, blank, R script is loaded, with
the name ``Untitled.R''. If you save this file (file
menu-\textgreater{}save), then you will be asked to give your new script
a name. Give it a new name. If you are working in an R Project, then
R-studio will automatically save your new script in your R project
folder. All ``.R'' files a just plain text files.

\subsection{An example script}\label{an-example-script}

After you create a new script, you can click into the editor window, and
write anything you want, just like a word processor. In general, the
scripts we will write, will give R instructions one line at a time.
Below is an example:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# this is a comment}
\NormalTok{a <-}\StringTok{ }\DecValTok{1+1}
\NormalTok{b <-}\StringTok{ }\DecValTok{2}\NormalTok{*}\DecValTok{3}
\NormalTok{c <-}\StringTok{ }\NormalTok{a+b}
\end{Highlighting}
\end{Shaded}

You can run this entire script in a few different ways:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Highlight all of the lines of text, copy them to the clipboard, then
  paste them into the console, and press enter (or return).
\item
  Highlight all of the lines of text, and press the ``run'' button at
  the top of the editor window (this automatically copies and pastes the
  selected lines, and runs them in the console).
\end{enumerate}

After you run the script, you should see some output in the R console.
Specifically, you should see each of lines of code that you asked R to
run.

Notice, however, that we do not see any of the answers of our this
script. What has happened?

Let's step through each line. The first line says ``\# this is a
comment''. Anything text that follows a ``\#'' tells R not to run that
line as code. Instead, R knows this is just a comment. Comments are very
useful to insert into your scripts to explain, in plain english what is
going on. For example, I will add more comments to the above script to
explain what is going on.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# this is a comment}
\NormalTok{a <-}\StringTok{ }\DecValTok{1+1} \CommentTok{# puts 1+1 into new variable a}
\NormalTok{b <-}\StringTok{ }\DecValTok{2}\NormalTok{*}\DecValTok{3} \CommentTok{# puts 2 times 3 into new variable b}
\NormalTok{c <-}\StringTok{ }\NormalTok{a+b }\CommentTok{# puts the sum of variable a and b into c}
\end{Highlighting}
\end{Shaded}

The comments give more insight into what R is doing here. For example,
each line does a simple calculation, and stores the result into a new
variable. Variables are a way to store our data in R. You can think of
them as containers with names.

Let's look more closely at this line:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{a <-}\StringTok{ }\DecValTok{1+1}
\end{Highlighting}
\end{Shaded}

\subsubsection{Variable name}\label{variable-name}

The `a' is the name of the variable. In general, you can choose any name
that you want. It is best to give descriptive names that are meaningful,
and that help you remember what the variable is being used for.

\subsubsection{\textless{}-}\label{section}

The `\textless{}-' command tells R to put something into the variable.
Anything that is to the right of the `\textless{}-' command will be put
into the variable named on the left-hand side of the `\textless{}-'
command

\subsubsection{1+1}\label{section-1}

1+1 is an operation that we are asking R to compute. The output of this
operation is put into `\textless{}-' the variable named `a'.

\subsubsection{Where are the variables?}\label{where-are-the-variables}

Where are these variables, and how can we see them? There are two ways
to see what is inside variables.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  In the top right window, you should see a tab called ``Environment''.
  This tab lists all of the variables that you have currently stored in
  R. You should see an a, b, and c, along with the numbers inside them.
\item
  You can type the name of the variable into the R console, and then
  press enter. The console will display the contents of the variable.
\end{enumerate}

\subsection{A bunch of numbers}\label{a-bunch-of-numbers}

Data comes in all shapes and sizes. Usually, there are so many numbers
that it is difficult to make sense of them. Let me show you 100 numbers.

Where did these numbers come from? What kind of properties do these
numbers have? Are there any patterns in the numbers? Do some kinds of
numbers happen more often than other kinds of numbers? What can we say
about these numbers just by looking at them?

If you take some time to look at the above numbers, you might start
noticing some regularities. When I quickly look at them I see:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Most the numbers are around 100.
\item
  The numbers all appear to be different by big or small amounts
\item
  There are no negative numbers
\item
  There are no really small numbers (e.g., close to 0)
\item
  There are no really huge numbers
\end{enumerate}

At least we can get some sense of the numbers by eyeballing them. If
there were 1000s or 100000s of numbers, eyeballing them one at a time
would take forever.

\subsection{Making numbers in R}\label{making-numbers-in-r}

Before we go ahead and use R to make plots and graphs of data, we need
to first have some data to plot. And, before we start using real data,
it is worth pointing out that we can use R to create numbers. So, after
we create our own sets of numbers, we can then plot them to see how
graphing works.

\subsubsection{rep function}\label{rep-function}

Let's say you wanted to create a variable that stored the number 43, 100
times. You can do this using the rep function (rep is short for repeat).
Below is an example of how this function is used.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my_numbers <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\DecValTok{43}\NormalTok{,}\DecValTok{100}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We can check to see what is inside the new variable \emph{my\_numbers}
by typing it into the console, or looking at it in the environment tab.
You should see that it contains the number 43, repeated 100 times. Using
the rep function you can repeat anything, any number of times

\subsubsection{seq function}\label{seq-function}

Let's say you want to create a sequence of numbers. You can do this
using the seq function. The example below starts at 23, and goes to 56,
in increments of 1. You can modify the starting value, the ending value,
and the increment value to create many different kinds of sequences.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my_sequence <-}\StringTok{ }\KeywordTok{seq}\NormalTok{(}\DecValTok{23}\NormalTok{,}\DecValTok{56}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsubsection{runif function}\label{runif-function}

R can generate numbers in much more sophisticated ways. In particular,
we can use R to sample numbers from distributions with particular
properties. We will introduce distributions in the next lab.

R can generate random numbers using the runif function. In the example
below, R generates 100 numbers that are randomly between 0 and 1. You
can generate as many numbers as you want, between any two numbers that
you want.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my_randoms <-}\StringTok{ }\KeywordTok{runif}\NormalTok{(}\DecValTok{100}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsubsection{rnorm function}\label{rnorm-function}

R can generate numbers from a normal distibrution. In the example belwo,
we generate 100 numbers from a distribution with a mean of 10, and a
standard deviation of 20.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my_normal <-}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{,}\DecValTok{10}\NormalTok{,}\DecValTok{20}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{Graphing Data in R}\label{graphing-data-in-r}

Graphs, or visual displays, of numbers can be very useful for
interpreting data. Fortunately, we can use R to create many kinds of
visual displays, that can help us interpret the data. To begin we will
look at the plot and histogram functions.

\subsubsection{Plot function}\label{plot-function}

Previously, we created a \emph{my\_numbers} variable, that contains the
number 43, repeated 100 times. If we plot this in R, what should we see?
Let's do it and find out.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(my_numbers)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Programming_Crump_files/figure-latex/unnamed-chunk-11-1.pdf}

Whenever you have a variable with multiple numbers, you can always plot
it, just like in the above example. Remember the variable
\emph{my\_numbers} contains 100 numbers. This means there are 100 slots
in the variable. Each slot has an \emph{index} value. The index value
for the first slot is 1, the index value for the second slot is 2, and
so on. The x-axis (the bottom line in the graph) shows the index value
from 1 to 100. Remember also, that each slot contains the number 43. The
y-axis (the vertical line in the graph) shows a range of numbers. The
dots in the graph represent the value inside each slot of the variable.
Because each slot contains the value 43, we see all 100 dots, all in a
line, all positioned at 43 with respect to the y-axis.

Let's plot some of the other variables we made.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(my_sequence)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Programming_Crump_files/figure-latex/unnamed-chunk-12-1.pdf}

The variable \emph{my\_sequence} contains the numbers 23 to 56, going up
by one. We see in the plot, the first number (on the x-axis) is a 23 on
the y-axis. As we go across the x-axis, the numbers go up by one until
we get to 56. We see a straight diagonal line.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(my_randoms)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Programming_Crump_files/figure-latex/unnamed-chunk-13-1.pdf}

The variable \emph{my\_randoms} contains 100 random numbers between 0
and 1. The plot shows dots all over the place between 0 and 1 on the
y-axis.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(my_normal)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Programming_Crump_files/figure-latex/unnamed-chunk-14-1.pdf}

The variable \emph{my\_normal} contains 100 numbers sample from a normal
distribution with a mean of 10, and a standard deviation of 20. Roughly,
most of the numbers should be close to 10, some of the numbers will be
greater and smaller than 10. But, as the numbers move away from 10 in
either direction, really small or really big numbers should occur less
and less frequently. We can sort of see this in the plot. For example,
you might notice that most the numbers are near the horizontal middle of
the graph, near the 10 on the y-axis, and less of the numbers are near
the top or bottom of the graph.

\subsubsection{Histograms}\label{histograms}

Histograms are used to visually summarize a set of numbers. In
particular, histograms split a set of numbers into bins, and then show
how many numbers fall within each bin. Each bin represents a pre-defined
range.

Let's create a set of numbers made up from 1s, 2s, and 3s. Let's say we
have ten 1s, twenty 2s, and 30 3s.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my_set <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\KeywordTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{10}\NormalTok{),}\KeywordTok{rep}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{20}\NormalTok{),}\KeywordTok{rep}\NormalTok{(}\DecValTok{3}\NormalTok{,}\DecValTok{30}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

The above line of code uses two R functions, rep(), and c(). We already
know how rep works. The c() function is short for combine. So, the above
line of code, combines 10 1s, 20 2s and 30 3s, all into one variable.
The contents of the variable looks like this:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my_set}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3
## [36] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3
\end{verbatim}

Because, we made this variable, we already know what is inside it. Let's
make a histogram of the variable, to see what that looks like:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{hist}\NormalTok{(my_set)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Programming_Crump_files/figure-latex/unnamed-chunk-17-1.pdf}

The histogram is a bar graph. The height of each bar represents a count,
or the frequency of how many numbers fall inside each bin. The x-axis
shows the bin ranges. If you do not specify the bin ranges, then R will
make a reasonable guess for you. In this case, R set the bin ranges in
steps of .5. For example, 1-1.5, 1.5-2, 2-2.5, 2.5-3. R uses the word
\emph{breaks} to refer to bins. And, when you plot a histogram, you can
set your own breaks, or bin ranges.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{hist}\NormalTok{(my_set, }\DataTypeTok{breaks=}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{4}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\includegraphics{Programming_Crump_files/figure-latex/unnamed-chunk-18-1.pdf}

Let's spend a moment interpreting this new histogram. The first bar is
between 0 and 1 on the x-axis, and has a value of 10 on the y-axis. This
means that there are 10 numbers inside the my\_set variable that have a
value between 0 and 1; specifically, a value greater than zero up to and
equalling 1. The second bar is between 1 and 2 on the x-axis, and has a
value of 20 on the y-axis. So, there are 20 numbers in the variable with
a value greater than 1 up to and equalling 2. Finally, the third bar
shows there are 30 numbers in the range greater than 2, up to equalling
3. We can also see there are no numbers smaller than 0, or greater than
4.

\subsubsection{What are histograms useful
for?}\label{what-are-histograms-useful-for}

A primary purpose of histograms is to get a quick look at the range and
frequency of a set of numbers. In particular, when the bars are of
different sizes, we can know that some values occur more than others.

What should a histogram look like for a set of values whose numbers all
occur randomly, and equally frequently? By this definition, we are
saying that all numbers, within all ranges, occur equally often. For
example, imagine we created a set of 10,000 numbers, and chose those
numbers from between 1 and 10, such that any number between 1 and 10
occurs with the same frequency as all other numbers. We can use the
random number generator and histogram to find out:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{some_random_numbers <-}\StringTok{ }\KeywordTok{runif}\NormalTok{(}\DecValTok{10000}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{10}\NormalTok{)}
\KeywordTok{hist}\NormalTok{(some_random_numbers,}\DataTypeTok{breaks=}\KeywordTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{11}\NormalTok{,}\DecValTok{1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\includegraphics{Programming_Crump_files/figure-latex/unnamed-chunk-19-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#notice I set the breaks using the seq() function}
\end{Highlighting}
\end{Shaded}

We see that heights of the bars are all close to the same number. This
is good, because each number between 1 to 10 should have had an equal
chance of being selected. However, notice the bars are not exactly the
same height. This shows that the random number generator did actually
generate each number with equal frequency.

What about sets of numbers where some kinds of numbers occur more than
others? Here, we would expect higher bars for ranges containing many
values, and smaller numbers for ranges containing fewer values.

Let's plot histogram for a normal distribution and see what it looks
like. We will set the mean to 100, and the standard deviation to 20, and
we ask R to generate 10000 numbers from this distribution.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{normal_sample <-}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(}\DecValTok{10000}\NormalTok{,}\DecValTok{100}\NormalTok{,}\DecValTok{20}\NormalTok{)}
\KeywordTok{hist}\NormalTok{(normal_sample)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Programming_Crump_files/figure-latex/unnamed-chunk-20-1.pdf}
The histogram shows that the highest bars are in the middle, near the
100 mark. So, numbers near 100 occured most frequently in our set. What
happens to the heights of the bars on either side of the histogram? The
bars are decreasing in height as they move away from the middle in both
directions. So, as numbers move away from 100, they occur less and less
frequently. For example, we don't see any bars in the range of 500 or
1000. This means that no values that high were in our set of numbers.
From the histogram, we can clearly see that our set hardly had any
numbers greater than 150, or less than 50. Or, in other words, most of
the numbers were between 50 and 150.

\section{Excel}\label{excel}

\section{SPSS}\label{spss}

\section{Matlab}\label{matlab}

\chapter{Lab 2: Descriptive
Statistics}\label{lab-2-descriptive-statistics}

{ Some inspiring quote ---Inspiring Person }

\section{Outline of Problem to solve}\label{outline-of-problem-to-solve}

Stuff we need to say in general

\subsection{important things}\label{important-things}

Other things to say

\section{R}\label{r-2}

How to do it in R

\section{Excel}\label{excel-1}

How to do it in Excel

\section{SPSS}\label{spss-1}

How to do it in SPSS

\section{Matlab}\label{matlab-1}

How to do it in Matlab

\chapter{Lab 3: Correlation}\label{lab-3-correlation}

{ Some inspiring quote ---Inspiring Person }

\section{Outline of Problem to
solve}\label{outline-of-problem-to-solve-1}

Stuff we need to say in general

\subsection{important things}\label{important-things-1}

Other things to say

\section{R}\label{r-3}

How to do it in R

\section{Excel}\label{excel-2}

How to do it in Excel

\section{SPSS}\label{spss-2}

How to do it in SPSS

\section{Matlab}\label{matlab-2}

How to do it in Matlab

\chapter{Lab 4: Normal Distribution \& Central Limit
Theorem}\label{lab-4-normal-distribution-central-limit-theorem}

{ Some inspiring quote ---Inspiring Person }

\section{Outline of Problem to
solve}\label{outline-of-problem-to-solve-2}

Stuff we need to say in general

\subsection{important things}\label{important-things-2}

Other things to say

\section{R}\label{r-4}

How to do it in R

\section{Excel}\label{excel-3}

How to do it in Excel

\section{SPSS}\label{spss-3}

How to do it in SPSS

\section{Matlab}\label{matlab-3}

How to do it in Matlab

\chapter{Lab 5: Fundamentals of Hypothesis
Testing}\label{lab-5-fundamentals-of-hypothesis-testing}

{ Some inspiring quote ---Inspiring Person }

\section{Outline of Problem to
solve}\label{outline-of-problem-to-solve-3}

Stuff we need to say in general

\subsection{important things}\label{important-things-3}

Other things to say

\section{R}\label{r-5}

How to do it in R

\section{Excel}\label{excel-4}

How to do it in Excel

\section{SPSS}\label{spss-4}

How to do it in SPSS

\section{Matlab}\label{matlab-4}

How to do it in Matlab

\chapter{Lab 6: t-Test (one-sample, paired
sample)}\label{lab-6-t-test-one-sample-paired-sample}

This lab is modified and extended from
\href{https://sites.trinity.edu/osl}{Open Stats Labs}. Thanks to Open
Stats Labs (Dr.~Kevin P. McIntyre) for their fantastic work.

\section{Does Music Convey Social Information to
Infants?}\label{does-music-convey-social-information-to-infants}

This lab activity uses the open data from Experiment 1 of Mehr, Song,
and Spelke (2016) to teach one-sample and paired samplest -tests.
Results of the activity provided below should exactly reproduce the
results described in the paper.

\subsection{STUDY DESCRIPTION}\label{study-description}

Parents often sing to their children and, even as infants, children
listen to and look at their parents while they are singing.Research by
Mehr, Song, and Spelke (2016) sought to explore the psychological
function that music has for parents and infants, by examining the
hypothesis that particular melodies convey important social information
to infants.Specifically, melodies convey information about social
affiliation.

The authors argue that melodies are shared within social groups. Whereas
children growing up in one culture may be exposed to certain songs as
infants (e.g., ``Rock-a-bye Baby''), children growing up in other
cultures (or even other groups within a culture) may be exposed to
different songs.Thus, when a novel person (someone who the infant has
never seen before) sings a familiar song, it may signal to the infant
that this new person is a member of their social group.

To test this hypothesis, the researchers recruited 32 infants and their
parents to complete an experiment.During their first visit to the lab,
the parents were taught a new lullaby (one that neither they nor their
infants had heard before).The experimenters asked the parents to sing
the new lullaby to their child every day for the next 1-2 weeks.

Following this 1-2 week exposure period, the parents and their infant
returned to the lab to complete the experimental portion of the
study.Infants were first shown a screen with side-by-side videos of two
unfamiliar people, each of whom were silently smiling and looking at the
infant.The researchers recorded the looking behavior (or gaze) of the
infants during this `baseline' phase. Next, one by one, the two
unfamiliar people on the screen sang either the lullaby that the parents
learned or a different lullaby (that had the same lyrics and rhythm, but
a different melody).Finally, the infants saw the same silent video used
at baseline, and the researchers again recorded the looking behavior of
the infants during this `test' phase.For more details on the
experiment's methods, please refer to Mehr et al. (2016) Experiment 1.

\section{Lab skills learned}\label{lab-skills-learned}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Conducting a one-sample t-test
\item
  Conducting a two-sample t-test
\item
  Plotting the data
\item
  Discussing inferences and limitations
\end{enumerate}

\section{Important Stuff}\label{important-stuff}

\begin{itemize}
\tightlist
\item
  citation: Mehr, S. A., Song. L. A., \& Spelke, E. S. (2016). For
  5-month-old infants, melodies are social. Psychological Science, 27,
  486-501.
\item
  \href{http://journals.sagepub.com/stoken/default+domain/d5HcBHg85XamSXGdYqYN/full}{Link
  to .pdf of article}
\item
  \href{https://drive.google.com/open?id=0Bz-rhZ21ShvOdW1wV0pmUTJSSk0}{Data
  in .csv format}
\item
  \href{https://drive.google.com/open?id=0Bz-rhZ21ShvOa3c4X3hqOWxwcUU}{Data
  in SPSS format}
\end{itemize}

\section{R}\label{r-6}

\subsection{Loading the data}\label{loading-the-data}

The first thing to do is download the .csv formatted data file, using
the link above, or just click
\href{https://drive.google.com/open?id=0Bz-rhZ21ShvOdW1wV0pmUTJSSk0}{here}.
It turns out there are lots of ways to load .csv files into R.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Load the data.table library. Then use the fread function and supply
  the web address to the file. Just like this. No downloading required.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(data.table)}
\NormalTok{all_data <-}\StringTok{ }\KeywordTok{fread}\NormalTok{(}\StringTok{"https://raw.githubusercontent.com/CrumpLab/statisticsLab/master/data/MehrSongSpelke2016.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Or, if you downloaded the .csv file. Then you can use fread, but you
  need to point it to the correct file location. The file location in
  this next example will not work for you, because the file is on my
  computer.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(data.table)}
\NormalTok{all_data <-}\StringTok{ }\KeywordTok{fread}\NormalTok{(}\StringTok{"Data/MehrSongSpelke2016.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{Inspect the data frame}\label{inspect-the-data-frame}

When you have loaded data it's always a good idea to check out what it
looks like. You can look at all of the data in the environment tab on
the top right hand corner. The data should be in a variable called
\texttt{all\_data}. Clicking on \texttt{all\_data} will load it into a
viewer, and you can scroll around. This can be helpful to see things.
But, there is so much data, can be hard to know what you are looking
for.

\subsubsection{summarytools}\label{summarytools}

The summarytools packages give a quick way to summarize all of the data
in a data frame. Here's how. When you run this code you will see the
summary in the viewer on the bottom right hand side. There's a little
browser button (arrow on top of little window) that you can click to
expand and see the whole thing in a browser.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(summarytools)}
\KeywordTok{view}\NormalTok{(}\KeywordTok{dfSummary}\NormalTok{(all_data))}
\end{Highlighting}
\end{Shaded}

\subsection{Get the data for Experiment
one}\label{get-the-data-for-experiment-one}

The data contains all of the measurements from all five experiments in
the paper. By searching through the \texttt{all\_data} dataframe, you
should look for the variables that code for each experiment. For
example, the third column is called \texttt{exp1}, which stands for
experiment 1. Notice that it contains a series of 1s. If you keep
scrolling down, the 1s stop. These 1s identify the rows associated with
the data for Experiment 1. We only want to analyse that data. So, we
need to filter our data, and put only those rows into a new variable. We
do this with the \texttt{dplyr} library, using the \texttt{filter}
function.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(dplyr)}
\NormalTok{experiment_one <-}\StringTok{ }\NormalTok{all_data %>%}\StringTok{ }\KeywordTok{filter}\NormalTok{(exp1==}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Now if you look at the new variable \texttt{experiment\_one}, there are
only 32 rows of data. Much less than before. Now we have the data from
experiment 1.

\subsection{Baseline phase: Conduct a one sample
t-test}\label{baseline-phase-conduct-a-one-sample-t-test}

You first want to show that infants' looking behavior did not differ
from chance during the baseline trial. The baseline trial was 16 seconds
long. During the baseline, infants watched a video of two unfamilar
people, one of the left and one on the right. There was no sound during
the basline. Both of the actors in the video smiled directly at the
infant.

The important question was to determine whether the infant looked more
or less to either person. If they showed no preference, the infant
should look at both people about 50\% of the time. How could we
determine whether the infant looked at both people about 50\% of the
time?

The \texttt{experiment\_one} dataframe has a column called
\texttt{Baseline\_Proportion\_Gaze\_to\_Singer}. All of these values
show how the proportion of time that the infant looked to the person who
would later sing the familiar song to them. If the average of these
proportion is .5 across the infants, then we would have some evidence
that the infants were not biased at the beginning of the experiment.
However, if the infants on average had a bias toward the singer, then
the average proportion of the looking time should be different than .5.

Using a one-sample t-test, we can test the hypothesis that our sample
mean for the \texttt{Baseline\_Proportion\_Gaze\_to\_Singer} was not
different from .5.

To do this in R, we just need to isolate the column of data called
\texttt{Baseline\_Proportion\_Gaze\_to\_Singer}. We will do this using
the \texttt{\$} operator. The \texttt{\$} operator is placed after any
data frame variable, and allows you to select a column of the data. The
next bit of code will select the column of data we want, and put it into
a new variable called \texttt{Baseline}. Note, if you type
\texttt{exp1\$} then Rstudio should automatically bring up all the
columns you can choose from.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{baseline <-}\StringTok{ }\NormalTok{experiment_one$Baseline_Proportion_Gaze_to_Singer}
\end{Highlighting}
\end{Shaded}

\subsubsection{Look at the numbers}\label{look-at-the-numbers}

\textbf{Question:} Why is it important to look at your numbers? What
could happen if you didn't?

Ok, we could just do the t-test right away, it's really easy, only one
line of code. But, we haven't even looked at the numbers yet. Let's at
least do that. First, we'll just use plot. It will show every data point
for each infant as a dot.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(baseline)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Programming_Crump_files/figure-latex/unnamed-chunk-26-1.pdf}

That's helpful, we see that the dots are all over the place. Let's do a
histrogram, so we can get a better sense of the frequency of different
proportions.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{hist}\NormalTok{(baseline)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Programming_Crump_files/figure-latex/unnamed-chunk-27-1.pdf}

\subsubsection{Look at the descriptives}\label{look-at-the-descriptives}

Let's get the mean and standard deviation of the sample

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mean}\NormalTok{(baseline)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.5210967
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sd}\NormalTok{(baseline)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.1769651
\end{verbatim}

Ok, so just looking at the mean, we see the proportion is close to .5
(it's .521). And, we see there is a healthy amount of variance (the dots
were all over the place), as the standard deviation was about .176.

\textbf{Question:} Based on the means and standard deviations can you
make an educated guess about what the t and p values might be? Learn how
to do this and you will be improving your data-sense.

Now, before we run the t-test, what do you think is going to happen? We
are going to get a t-value, and an associated p-value. If you can make a
guess at what those numbers would be right now in your head, and those
end up being pretty close to the ones we will see in a moment, then you
should pat yourself on the back. You have learned how to have intuitions
about the data. As I am writing this I will tell you that 1) I can't
remember what the t and p was from the paper, and I haven't done the
test yet, so I don't know the answer. So, I am allowed to guess what the
answer will be. Here are my guesses t(31) = 0.2, p = .95. The numbers in
the brackets are degrees of freedom, which we know are 31 (df= n-1 =
32-1= 31). More important than the specific numbers I am guessing (which
will probably be wrong), I am guessing that the p-value will be pretty
large, it will not be less than .05, which the author's set as their
alpha rate. So, I am guessing we will not reject the hypothesis that .52
is different from .5.

Let's do the t-test and see what happens.

\subsubsection{Conduct t.test}\label{conduct-t.test}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{t.test}\NormalTok{(baseline, }\DataTypeTok{mu=}\NormalTok{.}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  One Sample t-test
## 
## data:  baseline
## t = 0.67438, df = 31, p-value = 0.5051
## alternative hypothesis: true mean is not equal to 0.5
## 95 percent confidence interval:
##  0.4572940 0.5848994
## sample estimates:
## mean of x 
## 0.5210967
\end{verbatim}

\textbf{Question:} Why was the baseline condition important for the
experiment? What does performance in this condition tell us?

So, there we have it. We did a one-sample t-test. Here's how you would
report it, t(31) = .67, p = .505. Or, we might say something like:

\begin{quote}
During the baseline condition, the mean proportion looking time toward
the singer was .52, and was not significantly different from .5,
according to a one-sample test, t(31) = .67, p = .505.
\end{quote}

You should take the time to check this result, and see if it is the same
one that was reported in the paper.

\subsection{Test phase}\label{test-phase}

Remember how the experiment went. Infants watched silent video
recordings of two women (Baseline). Then each person sung a song, one
was familiar to the infant (their parents sung the song to them many
times), and one was unfamiliar (singing phase). After the singing phase,
the infants watched the silent video of the two singers argain (test
phase). The critical question was whether the infants would look more to
the person who sung the familiar song compared to the person who sun the
unfamiliar song. If the infants did this, they should look more than
50\% of the time to the singer who sang the familiar song. We have the
data, we can do another one sample t-test to find out. We can re-use all
the code we already wrote to do this. I'll put it all in one place. If
we run all of this code, we will see all of the things we want to see.

We only need to make two changes. We will change
\texttt{experiment\_one\$Baseline\_Proportion\_Gaze\_to\_Singer} to
\texttt{experiment\_one\$Test\_Proportion\_Gaze\_to\_Singer}, because
that column has the test phase data. And, instead of putting the data
into the variable \texttt{baseline}. We will make a new variable called
\texttt{test\_phase} to store the data.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{test_phase <-}\StringTok{ }\NormalTok{experiment_one$Test_Proportion_Gaze_to_Singer}
\KeywordTok{plot}\NormalTok{(test_phase)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Programming_Crump_files/figure-latex/unnamed-chunk-30-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{hist}\NormalTok{(test_phase)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Programming_Crump_files/figure-latex/unnamed-chunk-30-2.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mean}\NormalTok{(test_phase)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.5934913
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sd}\NormalTok{(test_phase)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.1786884
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{t.test}\NormalTok{(test_phase, }\DataTypeTok{mu =} \NormalTok{.}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  One Sample t-test
## 
## data:  test_phase
## t = 2.9597, df = 31, p-value = 0.005856
## alternative hypothesis: true mean is not equal to 0.5
## 95 percent confidence interval:
##  0.5290672 0.6579153
## sample estimates:
## mean of x 
## 0.5934913
\end{verbatim}

\textbf{Question:} Why was the test condition important for the
experiment? What does performance in this condition tell us?

Alright. What did we find? You should take a stab at writing down what
we found. You can use the same kind of language that I used from the
first one sample-test. You should state the mean proportion, the
t-value, the dfs, and the p-value. You should be able to answer the
question, did the infants look longer at the singer who sang the
familiar song? And, did they look longer than would be consist with
chance at 50\%.

\subsection{Paired-samples t-test}\label{paired-samples-t-test}

The paired samples t-test is easy to do. We've already made two
variables called \texttt{baseline}, and \texttt{test\_phase}. These
contain each of the infants looking time proportions to the singer for
both parts of the experiment. We can see if the difference between them
was likely or unlikely due to chance by running a paired samples t-test.
We do it like this in one line:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{t.test}\NormalTok{(test_phase, baseline, }\DataTypeTok{paired=}\OtherTok{TRUE}\NormalTok{, }\DataTypeTok{var.equal=}\OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Paired t-test
## 
## data:  test_phase and baseline
## t = 2.4164, df = 31, p-value = 0.02175
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  0.01129217 0.13349698
## sample estimates:
## mean of the differences 
##              0.07239458
\end{verbatim}

\textbf{Question:} Why was the paired samples t-test necessary if we
already did two one sample t-test? What new question is the paired
samples t-test asking?

I'l leave it to you to interpret these values, and to see if they are
the same as the ones in the paper. Based on these values what do you
conclude? Is there a difference between the mean proportion looking
times for the baseline and testing phase?

\subsubsection{Relationship between one-sample and paired sample
t-test}\label{relationship-between-one-sample-and-paired-sample-t-test}

\textbf{Question:} Why is it that a paired samples t-test can be the
same as the one sample t-test? What do you have to do the data in the
paired samples t-test in order to conduct a one-sample t-test that would
give you the same result?

We've discussed in the textbook that the one-sample and paired sample
t-test are related, they can be the same test. The one-sample test
whether a sample mean is different from some particular mean. The paired
sample t-test, is to determine whether one sample mean is different from
another sample mean. If you take the scores for each variable in a
paired samples t-test, and subtract them from one another, then you have
one list of difference scores. Then, you could use a one sample t-test
to test whether these difference scores are different from 0. It turns
out you get the same answer from a paired sample t-test testing the
difference between two sample means, and the one sample t-test testing
whether the mean difference of the difference scores between the samples
are different from 0. We can show this in r easily like this:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{t.test}\NormalTok{(test_phase, baseline, }\DataTypeTok{paired=}\OtherTok{TRUE}\NormalTok{, }\DataTypeTok{var.equal=}\OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Paired t-test
## 
## data:  test_phase and baseline
## t = 2.4164, df = 31, p-value = 0.02175
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  0.01129217 0.13349698
## sample estimates:
## mean of the differences 
##              0.07239458
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{difference_scores<-test_phase-baseline}
\KeywordTok{t.test}\NormalTok{(difference_scores, }\DataTypeTok{mu=}\DecValTok{0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  One Sample t-test
## 
## data:  difference_scores
## t = 2.4164, df = 31, p-value = 0.02175
## alternative hypothesis: true mean is not equal to 0
## 95 percent confidence interval:
##  0.01129217 0.13349698
## sample estimates:
##  mean of x 
## 0.07239458
\end{verbatim}

\subsubsection{Usefulness of difference
scores}\label{usefulness-of-difference-scores}

Ok fine, the paired samples t-test can be a one sample t-test if you use
the difference scores. This might just seem like a mildly useful factoid
you can use for stats trivia games (which no one plays). The point of
drawing your attention to the relationship, is to get you to focus on
the difference scores. These are what we are actually interested in.

Let's use the difference scores to one more useful thing. Sometime the
results of a t-test aren't intuitively obvious. By the t-test we found
out that a small difference between the test phase and baseline was not
likely produced by chance. How does this relate to the research question
about infants using familiar songs as cues for being social? Let's ask a
very simple question. How many infants actually showed the bias? How
many infants out of 32 looked longer at the singer who sang the familiar
song during test, compared to during baseline.

We can determine this by calculating the difference scores. Then, asking
how many of them were greater than zero:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{difference_scores <-}\StringTok{ }\NormalTok{test_phase-baseline}
\KeywordTok{length}\NormalTok{(difference_scores[difference_scores>}\DecValTok{0}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 22
\end{verbatim}

So, 22 out of 32 infants showed the effect. To put that in terms of
probability, 68.75\% of infants showed the effect. These odds and
percentages give us another way to appreciate how strong the effect is.
It wasn't strong enough for all infants to show it.

\subsection{Graphing the findings}\label{graphing-the-findings}

It is often useful to graph the results of our analysis. We have already
looked at dot plots and histograms of the individual samples. But, we
also conducted some t-tests on the means of the baseline and test\_phase
samples. One of the major questions was whether these means are
different. Now, we will make a graph that shows the means for each
condition. Actually, we will make a few different graphs, so that you
can think about what kinds of graphs are most helpful. There are two
major important things to show: 1) the sample means, and 2) a visual aid
for statistical inference showing whether the results were likely due to
chance.

We will use the ggplot2 package to make our graphs. Remember, there are
two steps to take when using ggplot2. 1) put your data in a long form
dataframe, where each measure has one row, and 2) define the layers of
the ggplot.

\subsubsection{Make the dataframe for
plotting}\label{make-the-dataframe-for-plotting}

To start we will need 2 columns. One column will code the experimental
phase, Baseline or Test. There are 32 observations in each phase, so we
want the word \texttt{Baseline} to appear 32 times, followed by the word
\texttt{Test} 32 times. Then we want a single column with each of the
proportions for each infant.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Phase <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"Baseline"}\NormalTok{,}\StringTok{"Test"}\NormalTok{), }\DataTypeTok{each =} \DecValTok{32}\NormalTok{)}
\NormalTok{Proportions <-}\StringTok{ }\KeywordTok{c}\NormalTok{(baseline,test_phase)}
\NormalTok{plot_df <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(Phase,Proportions)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Dot plot of raw scores}\label{dot-plot-of-raw-scores}

This shows every scores value on the y-axis, split by the baseline and
test groups. If you just looked at this, you might not think the test
phase was different from the basline phase. Still very useful to see the
spread of individual scores. Supports the intuition that the scores are
still kind of in a similar ballpark.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(ggplot2)}
\KeywordTok{ggplot}\NormalTok{(plot_df, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{Phase, }\DataTypeTok{y=}\NormalTok{Proportions))+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{Programming_Crump_files/figure-latex/unnamed-chunk-35-1.pdf}

\subsubsection{Dot plot with means and raw
scores}\label{dot-plot-with-means-and-raw-scores}

\textbf{Question:} What kinds of inferences about the role of chance in
producing the difference between the means can we make from this graph?
What is missing?

\texttt{ggplot2} is great because it let's us add different layers on
top of an existing plot. It would be good to see where the mean values
for each sample lie on top of the sample scores. We can do this. But, to
do it, we need to supply ggplot with another data frame, one that
contains the means for each phase in long form. There are are only two
phases, and two means, so we will make a rather small data.frame. It
will have two columns, a Phase column, and Mean\_value column. There
will only be two rows in the dataframe, one for the mean for each phase.

To make the smaller data frame for the means we will use the
\texttt{aggregate} function. This allows us to find the means for each
phase from the plot\_df dataframe. It also automatically returns the
data frame we are looking for.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mean_df <-}\StringTok{ }\KeywordTok{aggregate}\NormalTok{(Proportions ~}\StringTok{ }\NormalTok{Phase, plot_df, mean)}

\KeywordTok{ggplot}\NormalTok{(plot_df, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{Phase, }\DataTypeTok{y=}\NormalTok{Proportions))+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_point}\NormalTok{()+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{data=}\NormalTok{mean_df, }\DataTypeTok{color=}\StringTok{"Red"}\NormalTok{, }\DataTypeTok{size=}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Programming_Crump_files/figure-latex/unnamed-chunk-36-1.pdf}

\subsubsection{Bar plot}\label{bar-plot}

It's very common to use bars in graphs. We can easily do this by using
\texttt{geom\_bar}, rather than \texttt{geom\_point}. Also, we can plot
bars for the means, and keep showing the dots like this\ldots{}(note
this will be messed up, but I want to show you why).

Also look for these changes.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  added \texttt{stat="identity"} Necessary for bar plot to show specific
  numbers
\item
  added \texttt{aes(fill=Phase)} Makes each bar a different color,
  depending on which phase it comes from
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(plot_df, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{Phase, }\DataTypeTok{y=}\NormalTok{Proportions))+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_point}\NormalTok{()+}
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{(}\DataTypeTok{data=}\NormalTok{mean_df, }\DataTypeTok{stat=}\StringTok{"identity"}\NormalTok{,}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{fill=}\NormalTok{Phase))}
\end{Highlighting}
\end{Shaded}

\includegraphics{Programming_Crump_files/figure-latex/unnamed-chunk-37-1.pdf}

Ok, we see the bars and some of the dots, but not all of them. What is
going on? Remember, ggplot2 works in layers. Whatever layer you add
first will be printed first in the graph, whatever layer you add second
will be printed on top of the first. We put the bars on top of the dots.
Let's change the order of the layers so the dot's go on top of the bars.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(plot_df, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{Phase, }\DataTypeTok{y=}\NormalTok{Proportions))+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{(}\DataTypeTok{data=}\NormalTok{mean_df, }\DataTypeTok{stat=}\StringTok{"identity"}\NormalTok{,}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{fill=}\NormalTok{Phase))+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{Programming_Crump_files/figure-latex/unnamed-chunk-38-1.pdf}

\subsubsection{Bar plot with error bars}\label{bar-plot-with-error-bars}

So far we have only plotted the means and individual sample scores.
These are useful, but neither of them give us clear visual information
about our statistical test. Our paired sample t-test suggested that the
mean difference between Baseline and Test was not very likely by chance.
It could have happened, but wouldn't happen very often.

\textbf{Question:} Why would the standard deviation of each mean, or the
standard error of each mean be inappropriate to use in this case?

\textbf{Question:} How would error bars based on the standard error of
the mean differences aid in visual inference about the role of chance in
producing the difference?

Error bars are commonly used as an aid for visual inference. The use of
error bars can be a subtle nuanced issue. This is because there a
different kinds of error bars that can be plotted, and each one supports
different kinds of inference. In general, the error bar is supposed to
represent some aspect of the variability associated with each mean. We
could plot little bars that are +1 or -1 standard deviations of each
mean, or we would do +1 or -1 standard errors of each mean. In the case
of paired samples, neither or these error bars would be appropriate,
they wouldn't reflect the variability associated with mean we are
interested in. In a paired samples t-test, we are interested in the
variability of the mean of the difference scores. Let's calculate the
standard error of the mean (SEM) for the difference scores between
Baseline and Test, and then add error bars to the plot.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{difference_scores <-}\StringTok{ }\NormalTok{baseline-test_phase }\CommentTok{#calculate difference scores}
\NormalTok{standard_error <-}\StringTok{ }\KeywordTok{sd}\NormalTok{(difference_scores)/}\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{length}\NormalTok{(difference_scores)) }\CommentTok{#calculate SEM}


\KeywordTok{ggplot}\NormalTok{(plot_df, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{Phase, }\DataTypeTok{y=}\NormalTok{Proportions))+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{(}\DataTypeTok{data=}\NormalTok{mean_df, }\DataTypeTok{stat=}\StringTok{"identity"}\NormalTok{,}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{fill=}\NormalTok{Phase))+}
\StringTok{  }\KeywordTok{geom_errorbar}\NormalTok{(}\DataTypeTok{data=}\NormalTok{mean_df, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{ymin=}\NormalTok{Proportions-standard_error, }
                                  \DataTypeTok{ymax=}\NormalTok{Proportions+standard_error), }\DataTypeTok{width=}\NormalTok{.}\DecValTok{1}\NormalTok{) +}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{alpha=}\NormalTok{.}\DecValTok{25}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Programming_Crump_files/figure-latex/unnamed-chunk-39-1.pdf}

\textbf{Question:} What is one reason why these error bars (standard
error of the mean difference between Baseline and Test) are appropriate
to use. What is one reason they are not appropriate to use?

We have done something that is useful for visual inference. From the
textbook, we learned that differences of about 2 standard errors of the
mean are near the point where we would claim that chance is unlikely to
have produced the difference. This is a rough estimate. But, we can see
that the top of the error bar for Baseline is lower than the bottom of
the error bar for Test, resulting in a difference greater than 2
standard error bars. So, based on this graph, we might expect the
difference between conditions to be significant. We can also complain
about what we have done here, we are placing the same error bars from
the meand difference scores onto the means for each condition. In some
sense this is misleading. The error bars are not for the depicted sample
means, they are for the hidden single set of difference scores. To make
this more clear, we will make a bar plot with a single bar only for the
differences scores.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{difference_scores <-}\StringTok{ }\NormalTok{test_phase-baseline }\CommentTok{#calculate difference scores}
\NormalTok{standard_error <-}\StringTok{ }\KeywordTok{sd}\NormalTok{(difference_scores)/}\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{length}\NormalTok{(difference_scores)) }\CommentTok{#calculate SEM}
\NormalTok{mean_difference <-}\StringTok{ }\KeywordTok{mean}\NormalTok{(difference_scores)}

\KeywordTok{qplot}\NormalTok{(}\DataTypeTok{x=}\StringTok{"MeanDifference"}\NormalTok{, }\DataTypeTok{y=}\NormalTok{mean_difference)+}
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{(}\DataTypeTok{stat=}\StringTok{"identity"}\NormalTok{, }\DataTypeTok{width=}\NormalTok{.}\DecValTok{5}\NormalTok{, }\DataTypeTok{alpha=}\NormalTok{.}\DecValTok{5}\NormalTok{)+}
\StringTok{  }\KeywordTok{geom_hline}\NormalTok{(}\DataTypeTok{yintercept=}\DecValTok{0}\NormalTok{)+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{y=}\NormalTok{difference_scores), }\DataTypeTok{alpha=}\NormalTok{.}\DecValTok{25}\NormalTok{)+}
\StringTok{  }\KeywordTok{geom_errorbar}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{ymin=}\NormalTok{mean_difference-standard_error, }
                                  \DataTypeTok{ymax=}\NormalTok{mean_difference+standard_error), }\DataTypeTok{width=}\NormalTok{.}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Programming_Crump_files/figure-latex/unnamed-chunk-40-1.pdf}

\textbf{Question:} Why is is it more appropriate to put the standard
error of the difference on this bar graph? What important aspects of the
original results shown in the previous graph are missing from this
graph?

This plot is very useful too, it gives us some new information. We can
see that the mean difference (test - baseline) was greater than 0. And,
we are plotting the standard error of the mean differences, which are
the error bars that more formally belong to this graph. Still, we are in
a position to make some guesses for visual inference. The lower error
bar represents only 1 SEM. It does not cross 0, but the fact that 1 SEM
does not cross zero isn't important. For SEMs it's usually closer to 2.
We can sort of visually guesstimate that that 2 SEMs would not cross
zero, which would suggest we would obtain a small p-value. We also see
each of the mean difference scores. It's very clear that they are all
over the place. This means that not every infant showed a looking time
preference toward the singer of the familiar song. Finally, this single
bar plot misses something. It doesn't tell us what the values of the
original means were.

\subsubsection{Bar plot with confidence
intervals}\label{bar-plot-with-confidence-intervals}

Confidence intervals are also often used for error bars, rather than the
standard error (or other measure of variance). If we use 95\% confidence
intervals, then our error bars can be even more helpful for visual
inference. Running the \texttt{t.test} function produces confidence
interval estimates, and we can pull them out and use them for error
bars.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{t_test_results <-}\StringTok{ }\KeywordTok{t.test}\NormalTok{(difference_scores)}
\NormalTok{lower_interval<-}\StringTok{ }\NormalTok{t_test_results$conf.int[}\DecValTok{1}\NormalTok{]}
\NormalTok{upper_interval<-}\StringTok{ }\NormalTok{t_test_results$conf.int[}\DecValTok{2}\NormalTok{]}

\KeywordTok{qplot}\NormalTok{(}\DataTypeTok{x=}\StringTok{"MeanDifference"}\NormalTok{, }\DataTypeTok{y=}\NormalTok{mean_difference)+}
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{(}\DataTypeTok{stat=}\StringTok{"identity"}\NormalTok{, }\DataTypeTok{width=}\NormalTok{.}\DecValTok{5}\NormalTok{, }\DataTypeTok{alpha=}\NormalTok{.}\DecValTok{5}\NormalTok{)+}
\StringTok{  }\KeywordTok{geom_hline}\NormalTok{(}\DataTypeTok{yintercept=}\DecValTok{0}\NormalTok{)+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{y=}\NormalTok{difference_scores), }\DataTypeTok{alpha=}\NormalTok{.}\DecValTok{25}\NormalTok{)+}
\StringTok{  }\KeywordTok{geom_errorbar}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{ymin=}\NormalTok{lower_interval, }
                                  \DataTypeTok{ymax=}\NormalTok{upper_interval), }\DataTypeTok{width=}\NormalTok{.}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Programming_Crump_files/figure-latex/unnamed-chunk-41-1.pdf}

Notice that the 95\% confidence intervals around the mean are wider than
the SEM error bars from the previous graph. These new confidence
intervals tell us that 95\% of the time our sample mean will fall
between the two lines. The bottom line is slightly above 0, so we can
now visually see support for our statistical inference that chance was
unlikely to produce the result. If chance was likely to produce the
result, the horizontal line indicating 0, would be well inside the
confidence interval. We can also notice that the mean difference was
just barely different from chance, that lower bar is almost touching 0.

\subsection{Data-simulation}\label{data-simulation}

We can do a little bit of data simulation to get a better feel for this
kind of data. For example, we saw that the dots in our plots were quite
variable. We might wonder about what chance can do in the current
experiment. One way to do this is to estimate the standard deviation of
the looking time proportions. Perhaps the best way to do that would be
to an average of the standard deviation in the baseline and test\_phase
conditions. Then, we could simulate 32 scores from a normal distribution
with mean = .5, and standard deviation equalling our mean standard
deviation. We could calculate the mean of our simulated sample. And, we
could do this many times, say 1000 times. Then, we could look at a
histogram of our means. This will show the range of sample means we
would expect just by chance. This is another way to tell whether the
observed difference in this experiment in the testing phase was close or
not close from being produced by chance. Take a look at the histogram.
What do you think?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sample_sd   <-}\StringTok{ }\NormalTok{(}\KeywordTok{sd}\NormalTok{(baseline)+}\KeywordTok{sd}\NormalTok{(test_phase))/}\DecValTok{2}

\NormalTok{simulated_means <-}\StringTok{ }\KeywordTok{length}\NormalTok{(}\DecValTok{1000}\NormalTok{)}
\NormalTok{for(i in }\DecValTok{1}\NormalTok{:}\DecValTok{1000}\NormalTok{)\{}
 \NormalTok{simulated_means[i] <-}\StringTok{ }\KeywordTok{mean}\NormalTok{(}\KeywordTok{rnorm}\NormalTok{(}\DecValTok{32}\NormalTok{,.}\DecValTok{5}\NormalTok{, sample_sd))}
\NormalTok{\}}

\KeywordTok{hist}\NormalTok{(simulated_means)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Programming_Crump_files/figure-latex/unnamed-chunk-42-1.pdf}
\#\#\#\# Simulating the mean differences

We can do the simulation slightly differently to give us a different
look at chance. Above we simulated the sample means from a normal
distibrution centered on .5. The experimental question of interest was
whether the mean difference between the baseline and test\_phase
condition was different. So, we should do a simulation of the difference
scores. First, we estimate the standard deviation of the difference
scores, and then run the simulation with a normal distribution centered
on 0 (an expected mean difference of 0). This shows roughly how often we
might expect mean differences of various sizes to occur. One major
limitation is that we likely had only a rough estimate of the true
standard deviation of these mean differences, after all there were only
32 of them, so we should take this with a grain of salt. Nevertheless,
the pattern in the simulation fits well with the observations in that we
already made from the data.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sample_sd   <-}\StringTok{ }\KeywordTok{sd}\NormalTok{(baseline-test_phase)}

\NormalTok{simulated_mean_difference <-}\StringTok{ }\KeywordTok{length}\NormalTok{(}\DecValTok{1000}\NormalTok{)}
\NormalTok{for(i in }\DecValTok{1}\NormalTok{:}\DecValTok{1000}\NormalTok{)\{}
 \NormalTok{simulated_mean_difference[i] <-}\StringTok{ }\KeywordTok{mean}\NormalTok{(}\KeywordTok{rnorm}\NormalTok{(}\DecValTok{32}\NormalTok{,}\DecValTok{0}\NormalTok{, sample_sd))}
\NormalTok{\}}

\KeywordTok{hist}\NormalTok{(simulated_mean_difference)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Programming_Crump_files/figure-latex/unnamed-chunk-43-1.pdf}

\section{Excel}\label{excel-5}

How to do it in Excel

\section{SPSS}\label{spss-5}

How to do it in SPSS

\section{Matlab}\label{matlab-5}

How to do it in Matlab

\chapter{Lab 7: t-test (Independent
Sample)}\label{lab-7-t-test-independent-sample}

This lab is modified and extended from
\href{https://sites.trinity.edu/osl}{Open Stats Labs}. Thanks to Open
Stats Labs (Dr.~Kevin P. McIntyre) for their fantastic work.

\section{Do you come across as smarter when people read what you say or
hear what you
say?}\label{do-you-come-across-as-smarter-when-people-read-what-you-say-or-hear-what-you-say}

\subsection{STUDY DESCRIPTION}\label{study-description-1}

Imagine you were a job candidate trying to pitch your skills to a
potential employer. Would you be more likely to get the job after giving
a short speech describing your skills, or after writing a short speech
and having a potential employer read those words? That was the question
raised by Schroeder and Epley (2015).The authors predicted that a
person's speech (i.e., vocal tone, cadence, and pitch) communicates
information about their intellect better than their written words (even
if they are the same words as in the speech).

To examine this possibility, the authors randomly assigned 39
professional recruiters for Fortune 500 companies to one of two
conditions. In the audio condition, participants listened to audio
recordings of a job candidate's spoken job pitch. In the transcript
condition, participants read a transcription of the job candidate's
pitch. After hearing or reading the pitch, the participants rated the
job candidates on three dimensions: intelligence, competence, and
thoughtfulness. These ratings were then averaged to create a single
measure of the job candidate's intellect, with higher scores indicating
the recruiters rated the candidates as higher in intellect. The
participants also rated their overall impression of the job candidate (a
composite of two items measuring positive and negative impressions).
Finally, the participants indicated how likely they would be to
recommend hiring the job candidate (0 - not at all likely, 10 -
extremely likely).

What happened? Did the recruiters think job applicants were smarter when
they read the transcripts, or when the heard the applicants speak? We
have the data, we can find out.

\section{Lab skills learned}\label{lab-skills-learned-1}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Conduct independent samples t-tests
\item
  Generate figures
\item
  Discuss the results and implications
\end{enumerate}

\section{Important Stuff}\label{important-stuff-1}

\begin{itemize}
\tightlist
\item
  citation: Schroeder, J., \& Epley, N. (2015). The sound of intellect:
  Speech reveals a thoughtful mind, increasing a job candidate's appeal.
  Psychological Science, 26, 877-891.
\item
  \href{http://journals.sagepub.com/stoken/default+domain/PhtK6MPtXvkgnYRrnGbA/full}{Link
  to .pdf of article}
\item
  \href{https://drive.google.com/open?id=0Bz-rhZ21ShvOei1MM24xNndnQ00}{Data
  in .csv format}
\item
  \href{https://drive.google.com/open?id=0Bz-rhZ21ShvOVXlDMjEzQU1oY1k}{Data
  in SPSS format}
\end{itemize}

\section{R}\label{r-7}

\subsection{Load the data}\label{load-the-data}

Remember that any line with a \# makes a comment and the code does not
run. Below is how to load the .csv data from the online repository, or
from a local file (you need to change the file path to where the local
file is, if you downloaded it). The data contains all of the measures
and conditions from Experiment 4.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(data.table)}
\CommentTok{# load from github repo}
\CommentTok{#all_data <- fread("https://raw.githubusercontent.com/CrumpLab/statisticsLab/master/data/SchroederEpley2015data.csv")}

\NormalTok{all_data <-}\StringTok{ }\KeywordTok{fread}\NormalTok{(}\StringTok{"Data/SchroederEpley2015data.csv"}\NormalTok{) }\CommentTok{# load from file on computer}
\end{Highlighting}
\end{Shaded}

\subsection{Inspect data frame}\label{inspect-data-frame}

This will give you a big picture of the data frame. Click the button to
view it in your browser, then take a look to see what is in it.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(summarytools)}
\KeywordTok{view}\NormalTok{(}\KeywordTok{dfSummary}\NormalTok{(all_data))}
\end{Highlighting}
\end{Shaded}

\subsection{Find the data you need}\label{find-the-data-you-need}

This time the data comes prefiltered for us. The authors ran lots of
experiments, but we only have the data from Experiment 4. This is great,
we don't need to subset the data frame to find all of the data that we
need. But, we do still need to understand what data we want to analyze.
Let's start with identify the column that codes the experimental
conditions for whether or not the evaluator read a transcript or heard
the interview.

\subsubsection{Condition variable}\label{condition-variable}

Lucky for us, the condition variable is called \texttt{CONDITION}! Let's
take a look. We printed it out just by writing down
\texttt{all\_data\$CONDITION}. There 0s and 1s for each condition (audio
vs.~transcript). But which one is which? This isn't clear from the data,
and it isn't clear from the paper, or from the repository. We have to do
some guess work. I went ahead and computed the means for the
Intellect\_rating between each condition, and then compared those to the
graph in the paper for E4. It looks like 1 = audio condition, and 0 =
transcript condition.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{all_data$CONDITION}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] 1 1 1 0 0 0 1 0 1 0 0 1 1 0 0 1 1 1 0 0 0 0 0 1 1 1 0 1 1 1 0 0 1 1 0
## [36] 1 0 1 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{aggregate}\NormalTok{(Intellect_Rating~CONDITION,all_data,mean)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   CONDITION Intellect_Rating
## 1         0         3.648148
## 2         1         5.634921
\end{verbatim}

Let's use words instead of 0s and 1s to refer to our experimental
conditions. To do this, we will chance the values of 0 and 1, to the
words \texttt{transcript} and \texttt{audio}. We can do this in two
steps. First we convert the CONDITION column to a factor. This will
automatically turn the 0s and 1s into strings (not numbers, text).
Factors have an internal variable for the names of the levels, which
will be 0 and 1. We can simply change the level names to transcript and
audio.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{all_data$CONDITION <-}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(all_data$CONDITION)}
\KeywordTok{levels}\NormalTok{(all_data$CONDITION) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"transcript"}\NormalTok{,}\StringTok{"audio"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Now if you look at the \texttt{all\_data} variable, you will see the
words transcript and audio, where 0s and 1s used to be.

\subsubsection{Dependent Measures}\label{dependent-measures}

Next it's time to find the depende measure columns. The graph from the
paper shows three different measures in each condition. These included
\texttt{Intellect}, \texttt{General\ Impression}, and
\texttt{Hiring\ Likelihood}. Every evaluator (either given a transcript
or audio recording of the interview) gave ratings on a scale of 1 to 10
for each of those concepts. It's not immediately clear which columns in
\texttt{all\_data} correspond to those three measures. There are lots of
different measures that could be the ones they reported. It turns out
the relevant ones are called

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{Intellect\_Rating}
\item
  \texttt{Impression\_Rating}
\item
  \texttt{Hire\_Rating}
\end{enumerate}

In this tutorial we are going to walk through doing an independent
samples t-test for the first measure, \texttt{Intellect\_Rating}. You
can follow these same steps to complete the same kind of t-test for the
other two variables.

\subsection{Look at the dependent
variable.}\label{look-at-the-dependent-variable.}

\textbf{Question:} Why do we always want to look at the data?

What is the first thing we do before even considering an inferential
test? Look at the data. Always look at the data. We could make a dot
plot or histogram of the data from the \texttt{Intellect\_ratings}. But,
from our last lab we already learned how to make graphs showing most of
the information we would want to look at. For example, we could make a
bar graph that has the means for each condition (transcript vs.~audio),
standard errors of the mean and the actual scores as little dots. This
would be great to look at it. Not only will it tell us if there are
really weird numbers in the data (who knows maybe the data file is
corrupted, you need to look), it will also give us strong intuitions
about what to expect for the t-test.

We can plot each score as a dot using the \texttt{all\_data} dataframe.
If we want to add on a layer for the sample means, and for the sample
standard errors, we have to compute those and put them in a new
dataframe first. Then we use both dataframes with ggplot to plot all of
the information.

We will use \texttt{dplyr} to quickly get the means and the standard
errors and put them in a new data frame called \texttt{decriptive\_df}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(dplyr)}
\KeywordTok{library}\NormalTok{(ggplot2)}

\CommentTok{# get means and SEs}
\NormalTok{descriptive_df <-}\StringTok{ }\NormalTok{all_data %>%}\StringTok{ }
\StringTok{                    }\KeywordTok{group_by}\NormalTok{(CONDITION) %>%}\StringTok{ }
\StringTok{                    }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{means=} \KeywordTok{mean}\NormalTok{(Intellect_Rating),}
                              \DataTypeTok{SEs =} \KeywordTok{sd}\NormalTok{(Intellect_Rating)/}\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{length}\NormalTok{(Intellect_Rating)))}

\CommentTok{# Make the plot}
\KeywordTok{ggplot}\NormalTok{(descriptive_df, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{CONDITION, }\DataTypeTok{y=}\NormalTok{means))+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{(}\DataTypeTok{stat=}\StringTok{"identity"}\NormalTok{, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{fill=}\NormalTok{CONDITION))+}\StringTok{ }\CommentTok{# add means}
\StringTok{  }\KeywordTok{geom_errorbar}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{ymin=}\NormalTok{means-SEs,               }\CommentTok{# add error bars}
                    \DataTypeTok{ymax=}\NormalTok{means+SEs), }\DataTypeTok{width=}\NormalTok{.}\DecValTok{1}\NormalTok{) +}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{data=}\NormalTok{all_data, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{CONDITION, }\DataTypeTok{y=}\NormalTok{Intellect_Rating), }\DataTypeTok{alpha=}\NormalTok{.}\DecValTok{5}\NormalTok{)+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{alpha=}\NormalTok{.}\DecValTok{25}\NormalTok{)+}
\StringTok{  }\KeywordTok{ylab}\NormalTok{(}\StringTok{"Rating"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Programming_Crump_files/figure-latex/unnamed-chunk-48-1.pdf}

This plot is very useful. First, we can see the numbers in our dependent
measure are behaving sensibly. We know that the numbers have to be
between 1-10, because those were the only options in the scale. If we
found numbers bigger or smaller, we would know something was wrong.
Checking for things that are obvsiouly wrong in the data is one reason
why we always look at first. We are checking for obvious errors. There
are other ways to check to, but looking is fast and easy.

\textbf{Question:} Why are the standard errors of each sample an
appropriate thing to use for error bars?

Now that you can see the patterns in the data, you should form an
intuition about how the independent samples t-test will turn out. You
can see how big the error bars (+1/-1 standard error of each sample
man). The t-test will tell us whether the observed difference (or
greater) is likely due to chance. Should we find a big t-value or a
small t-value? Should we find a big p-value or a small t-value. If you
understand how t-values and p-values work, the answer should be very
clear from the graph. You should already know how the t-test will turn
out before you run it. Running it will confirm what you already suspect
to be true.

\subsection{Conduct Independent samples
t-test}\label{conduct-independent-samples-t-test}

\textbf{Question:} Why are we conducting an independent samples t-test,
and not a one-sample or paired samples t-test?

We use the very same \texttt{t.test} function that we used last time to
conduct a t-test. The only difference is that we don't tell the R to use
a paired sample t-test. We leave the \texttt{paired=TRUE} statment out,
and R automatically knows we want to do an independent samples t-test.
Remember to set the \texttt{var.equal=TRUE}, otherwise R will compute a
different version of the t-test.

You can use different syntax to run the t-test. Because our data is
already in a data frame we can use this syntax.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{t.test}\NormalTok{(Intellect_Rating~CONDITION, }\DataTypeTok{data=}\NormalTok{all_data, }\DataTypeTok{var.equal=}\OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Two Sample t-test
## 
## data:  Intellect_Rating by CONDITION
## t = -3.5259, df = 37, p-value = 0.001144
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -3.1284798 -0.8450652
## sample estimates:
## mean in group transcript      mean in group audio 
##                 3.648148                 5.634921
\end{verbatim}

The \texttt{t.test} function also will work on two variables, not in a
data frame. For example, the following does the same thing. But, it's
harder to read, and the means are described in terms of X and Y, not
terms of \texttt{transcript} and \texttt{audio}, like the report above.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{t.test}\NormalTok{(all_data[all_data$CONDITION==}\StringTok{'transcript'}\NormalTok{,]$Intellect_Rating,}
       \NormalTok{all_data[all_data$CONDITION==}\StringTok{'audio'}\NormalTok{,]$Intellect_Rating,}
       \DataTypeTok{var.equal=}\NormalTok{T)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Two Sample t-test
## 
## data:  all_data[all_data$CONDITION == "transcript", ]$Intellect_Rating and all_data[all_data$CONDITION == "audio", ]$Intellect_Rating
## t = -3.5259, df = 37, p-value = 0.001144
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -3.1284798 -0.8450652
## sample estimates:
## mean of x mean of y 
##  3.648148  5.634921
\end{verbatim}

\textbf{Question:} What conclusions do we draw from the t-test? Based on
these results, if you were being evaluated for a job interview, would
you rather have the evaluator read a transcript of your interview or
listen to an audio recording?

So, now we have the t-test. It shows the t-value, the p-value, and the
means for each group. You can double-check with the paper to see if we
found the same results as reported by the authors.

\subsection{Remaining ratings}\label{remaining-ratings}

Now, you should use what you have learned to anlayse the last two
ratings for the dependent variables \texttt{Impression\_Rating}, and
\texttt{Hire\_Rating}. Remember to plot the data for each, and conduct a
t-test for each. Then compare what you found to the original article.
What did you find, and what do the results mean?

\subsection{Reconstructing the graph from the
paper}\label{reconstructing-the-graph-from-the-paper}

The results from Experimen4 4 in the paper plot the means and error bars
(+1 / -1 SEM) for all three dependent measures, for both experimental
conditions. We can do this in ggplot using the data. We will have to
make a couple changes to the dataframe. But, it won't be too hard. What
we need to do is make a fully long form data frame. Remember a long form
data frame has one row per dependent measure.

The \texttt{all\_data} frame is partly long and partly wide. If we are
only interested in one dependent measure, then it is a long data frame
for that measure. For example, example if we are only interested in
plotting \texttt{Intellect\_Rating}, then we already have one
observation of that dependent measure for each row. But, in the other
columns, the dependent measures for \texttt{Impression\_Rating} and
\texttt{Hire\_Rating} are in the same rows.

Before continuing, it is very much worth mentioning that this part of
data analysis happens alot, and it is kind of annoying. I call it the
rubix cube problem, because we need to ``rotate'' and transform the
format of the data to accomplish different kinds of analysis goals. It's
good to be able to know how to do this. This problem occurs all of the
time, can occur for any software package. It's a good thing you are
learning R, because we can do these things easily in R. They are not
often so easy to do without a computer programming language like R. The
worst thing to do is transform the data by hand. That really sucks.
Believe me you don't want to do it. Why? Because you will make mistakes,
and you will mess up the data, then you will mess up your analysis. And,
you won't be able to find your mistakes, and it will take you ages to
correct them. That sucks.

There's more than one way to transform data in R. For example the
\texttt{cast} and \texttt{melt} functions do this kind of thing. You can
look those up. In this example we will not use those functions. Instead
we will show some steps to build the required dataframe one step at a
time.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# repeat CONDITION column three times}

\NormalTok{condition <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(all_data$CONDITION,}\DecValTok{3}\NormalTok{)}

\CommentTok{# make a ratings variable with all three ratings in one variable}

\NormalTok{ratings <-}\StringTok{ }\KeywordTok{c}\NormalTok{(all_data$Intellect_Rating,}
             \NormalTok{all_data$Impression_Rating,}
             \NormalTok{all_data$Hire_Rating)}

\CommentTok{# make a new factor variable with the names of the ratings}
\CommentTok{# need to repeat each level name the appropriate number of times}

\NormalTok{num_to_repeat <-}\StringTok{ }\KeywordTok{length}\NormalTok{(all_data$CONDITION)}

\NormalTok{rating_type <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"Intellect"}\NormalTok{,}\StringTok{"Impression"}\NormalTok{,}\StringTok{"Hire"}\NormalTok{),num_to_repeat)}

\CommentTok{# put the new variables into a data frame}

\NormalTok{plot_all <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(condition,rating_type,ratings)}

\CommentTok{# Get the means and standard errors for each rating by condition}

\NormalTok{descriptive_all <-}\StringTok{ }\NormalTok{plot_all %>%}\StringTok{ }
\StringTok{                    }\KeywordTok{group_by}\NormalTok{(condition,rating_type) %>%}\StringTok{ }
\StringTok{                    }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{means=} \KeywordTok{mean}\NormalTok{(ratings),}
                              \DataTypeTok{SEs =} \KeywordTok{sd}\NormalTok{(ratings)/}\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{length}\NormalTok{(ratings)))}

\CommentTok{# Make the plot}

\KeywordTok{ggplot}\NormalTok{(descriptive_all, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{rating_type, }\DataTypeTok{y=}\NormalTok{means, }\DataTypeTok{group=}\NormalTok{condition))+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{(}\DataTypeTok{stat=}\StringTok{"identity"}\NormalTok{, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{fill=}\NormalTok{condition), }\DataTypeTok{position=}\StringTok{'dodge'}\NormalTok{)+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_errorbar}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{ymin=}\NormalTok{means-SEs,               }
                    \DataTypeTok{ymax=}\NormalTok{means+SEs), }
                \DataTypeTok{width=}\NormalTok{.}\DecValTok{1}\NormalTok{, }
                \DataTypeTok{position =} \KeywordTok{position_dodge}\NormalTok{(}\DataTypeTok{width=}\NormalTok{.}\DecValTok{9}\NormalTok{)) +}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{data=}\NormalTok{plot_all, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{rating_type, }
                                \DataTypeTok{y=}\NormalTok{ratings, }
                                \DataTypeTok{group=}\NormalTok{condition), }
             \DataTypeTok{alpha=}\NormalTok{.}\DecValTok{25}\NormalTok{, }
             \DataTypeTok{position =} \KeywordTok{position_dodge}\NormalTok{(}\DataTypeTok{width=}\NormalTok{.}\DecValTok{9}\NormalTok{))+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{alpha=}\NormalTok{.}\DecValTok{25}\NormalTok{)+}
\StringTok{  }\KeywordTok{ylab}\NormalTok{(}\StringTok{"Rating"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Programming_Crump_files/figure-latex/unnamed-chunk-51-1.pdf}

Well, we didn't make the exact graph. We have the bars, the error bars,
and we added the individual scores because they are useful to look at.
Otherwise, it's the same graph (except the the ordering of bars is
determined alphabetically here. We change that in ggplot, but we won't
do that today.)

\section{Excel}\label{excel-6}

How to do it in Excel

\section{SPSS}\label{spss-6}

How to do it in SPSS

\section{Matlab}\label{matlab-6}

How to do it in Matlab

\chapter{Lab 8: One-way ANOVA}\label{lab-8-one-way-anova}

This lab is modified and extended from
\href{https://sites.trinity.edu/osl}{Open Stats Labs}. Thanks to Open
Stats Labs (Dr.~Kevin P. McIntyre) for their fantastic work.

\section{Lab Skills Learned}\label{lab-skills-learned-2}

\section{Important Stuff}\label{important-stuff-2}

\begin{itemize}
\tightlist
\item
  citation: James, E. L., Bonsall, M. B., Hoppitt, L., Tunbridge, E. M.,
  Geddes, J. R., Milton, A. L., \& Holmes, E. A. (2015). Computer game
  play reduces intrusive memories of experimental trauma via
  reconsolidation-update mechanisms. Psychological Science, 26,
  1201-1215.
\item
  \href{http://journals.sagepub.com/stoken/default+domain/hQ2W4fbPrZVJ7eyNJaqu/full}{Link
  to .pdf of article}
\item
  \href{https://drive.google.com/open?id=0Bz-rhZ21ShvOM1cxWUpUNlQ0UlE}{Data
  in .csv format}
\item
  \href{https://drive.google.com/file/d/0Bz-rhZ21ShvOZ1lvQ0dQekZGWU0/view?usp=sharing}{Data
  in SPSS format}
\end{itemize}

\section{Outline of Problem to
solve}\label{outline-of-problem-to-solve-4}

Stuff we need to say in general

\subsection{important things}\label{important-things-4}

Other things to say

\section{R}\label{r-8}

How to do it in R

\section{Excel}\label{excel-7}

How to do it in Excel

\section{SPSS}\label{spss-7}

How to do it in SPSS

\section{Matlab}\label{matlab-7}

How to do it in Matlab

\chapter{Lab 9: One-way ANOVA}\label{lab-9-one-way-anova}

\section{Lab Skills Learned}\label{lab-skills-learned-3}

\section{Important Stuff}\label{important-stuff-3}

\begin{itemize}
\tightlist
\item
  citation: Rosenbaum, D., Mama, Y., \& Algom, D. (2017). Stand by Your
  Stroop: Standing Up Enhances Selective Attention and Cognitive
  Control. Psychological science, 28(12), 1864-1867.
\item
  \href{http://journals.sagepub.com/doi/abs/10.1177/0956797617721270?journalCode=pssa}{Link
  to .pdf of article}
\item
  \href{}{Data in .csv format}
\item
  \href{}{Data in SPSS format}
\end{itemize}

\section{Outline of Problem to
solve}\label{outline-of-problem-to-solve-5}

Stuff we need to say in general

\subsection{important things}\label{important-things-5}

Other things to say

\section{R}\label{r-9}

How to do it in R

\section{Excel}\label{excel-8}

How to do it in Excel

\section{SPSS}\label{spss-8}

How to do it in SPSS

\section{Matlab}\label{matlab-8}

How to do it in Matlab

\chapter{Lab 10: Factorial ANOVA}\label{lab-10-factorial-anova}

\section{Lab Skills Learned}\label{lab-skills-learned-4}

\section{Important Stuff}\label{important-stuff-4}

\begin{itemize}
\tightlist
\item
  citation: Barasch, A., Diehl, K., Silverman, J., \& Zauberman, G.
  (2017). Photographic memory: The effects of volitional photo taking on
  memory for visual and auditory aspects of an experience. Psychological
  science, 28(8), 1056-1066.
\item
  \href{http://journals.sagepub.com/doi/abs/10.1177/0956797617694868}{Link
  to .pdf of article}
\item
  \href{}{Data in .csv format}
\item
  \href{}{Data in SPSS format}
\end{itemize}

\section{Outline of Problem to
solve}\label{outline-of-problem-to-solve-6}

Stuff we need to say in general

\subsection{important things}\label{important-things-6}

Other things to say

\section{R}\label{r-10}

How to do it in R

\section{Excel}\label{excel-9}

How to do it in Excel

\section{SPSS}\label{spss-9}

How to do it in SPSS

\section{Matlab}\label{matlab-9}

How to do it in Matlab

\chapter{Lab 11: Mixed Factorial
ANOVA}\label{lab-11-mixed-factorial-anova}

\section{Lab Skills Learned}\label{lab-skills-learned-5}

\section{Important Stuff}\label{important-stuff-5}

\begin{itemize}
\tightlist
\item
  citation:
\item
  \href{}{Link to .pdf of article}
\item
  \href{}{Data in .csv format}
\item
  \href{}{Data in SPSS format} \#\# Outline of Problem to solve
\end{itemize}

Stuff we need to say in general

\subsection{important things}\label{important-things-7}

Other things to say

\section{R}\label{r-11}

How to do it in R

\section{Excel}\label{excel-10}

How to do it in Excel

\section{SPSS}\label{spss-10}

How to do it in SPSS

\section{Matlab}\label{matlab-10}

How to do it in Matlab

\bibliography{book.bib,packages.bib,MyLibrary.bib}


\end{document}
